# -*- coding: utf-8 -*-
"""ATSA_SVM_SBI.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16ORD4kWqGOoU8j1CixT_p-1hX2zV-IjB
"""

from pandas import read_csv
from pandas import datetime
from pandas import DataFrame
from statsmodels.tsa.arima_model import ARIMA
from matplotlib import pyplot as plt
from pandas.plotting import autocorrelation_plot
from sklearn.metrics import mean_squared_error
import numpy as np
from sklearn.svm import SVR
from sklearn.metrics import accuracy_score
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.multiclass import OneVsRestClassifier
import warnings
warnings.filterwarnings("ignore")

series = read_csv('/content/SBIN.NS.csv')
print (series)

from matplotlib import pyplot
series = np.array(series)
epsilon = 0.2 # if the difference between predicted value and true value>epsilon in ARIMA, feed into SVR
outlier = list()

size = 2491
train = series[0:size]
test =  series[size:len(series)]
true_data = list()
history = list()
predictions = list()
start_point = 10
for i in range(0,start_point):
    history.append(train[i])

for t in range(start_point,len(train)):
    model = ARIMA(history, order=(0,1,0)) # arima order means that it is a random walk
    model_fit = model.fit()
    output = model_fit.forecast()
    yhat = output[0]
    if abs(yhat-train[t]) > epsilon :
       outlier.append(train[t]) #if a point is outlier, put its index into the set
               

history = list()
history.append(outlier[0])

sum = 0

for t in range(1, len(outlier)):
    predicted_data = np.arange(len(history),len(history)+1)
    predicted_data = np.expand_dims(predicted_data,axis=1)
    train_data = np.arange(0,len(history))
    train_data = np.expand_dims(train_data,axis=1)
    svr = SVR(kernel='rbf', C=1e3, gamma = 1/1250)
    yhat = svr.fit(train_data,history).predict(predicted_data)
    true_data.append(outlier[t]) # replace train data into true data
    percentage = 1 - np.sqrt(np.mean(np.square((true_data - yhat) / true_data)))
    sum = sum + percentage

print("Accuracy : ", (sum/len(series))*100, " %")

# plt.xlabel('Last 20 days number')
# plt.ylabel('Closing price')
# plt.plot([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,18, 19, 20], [306.24094273393814, 298.93582946247506, 297.88738043002775, 311.49470617045796, 309.77614429129363, 295.29370523206046, 304.9302550797593, 315.2214213683863, 312.7705063476503, 294.4208456538196, 297.05613524706513, 297.5264615781839, 303.410048230613, 303.6669650722423, 298.85978155635894, 295.1496551384521, 306.3445874531751, 315.62318332657355, 313.756858753071, 315.7873067899246]
# , label='Original closing prices')
# plt.plot([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,18, 19, 20], [192.059998, 195.684998,194.240005,190.529999,191.634995,191.190002,192.125,191.365005,197.429993,198.664993, 202.240005,203.270004,204.720001,206.595001,204.460007,203.725006,204.574997,204.789993,201.509995,201.585007], label='Predicted closing prices')
# plt.legend()