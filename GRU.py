# -*- coding: utf-8 -*-
"""ATSA_Arima_GRU_SBI.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17RuHMoEG3S5_FdBGSSgT6A59hRwjA3Gx
"""

import pandas as pd
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt
import numpy as np
import math
from statsmodels.tsa.stattools import acf, pacf
import statsmodels.tsa.stattools as ts
from statsmodels.tsa.arima_model import ARIMA

from google.colab import drive
drive.mount('/content/drive')

variables = pd.read_csv("/content/SBIN.NS.csv")
Close =variables ["Close"]
Close

lnClose=np.log(Close)
lnClose
plt.xlabel('day number starting with 0')
plt.ylabel('log values')
plt.plot(lnClose)
plt.show()

acf_1 =  acf(lnClose)[1:20]
test_df = pd.DataFrame([acf_1]).T
test_df.columns = ['autocorreletion']
test_df.index += 1

test_df.plot(kind='bar')

plt.xlabel('lag')
plt.ylabel('autocorrelation coefficient')

plt.show()

pacf_1 = pacf(lnClose)[1:20]
test_df = pd.DataFrame([pacf_1]).T
test_df.columns = ['Partial Autocorrelation']
test_df.index += 1
test_df.plot(kind='bar')
plt.xlabel('lag')
plt.ylabel('partial autocorrelation coefficient')
plt.show

lnClose_diff=lnClose-lnClose.shift(1)
diff=lnClose_diff.dropna()
acf_1_diff =  acf(diff)[1:20]
test_df = pd.DataFrame([acf_1_diff]).T
test_df.columns = ['First Difference Autocorrelation']
test_df.index += 1
test_df.plot(kind='bar')
pacf_1_diff =  pacf(diff)[1:20]
plt.plot(pacf_1_diff)
plt.xlabel('lag')
plt.ylabel('partial autocorrelation coefficient')
plt.show()
plt.plot(acf_1_diff)
plt.xlabel('lag')
plt.ylabel('autocorrelation coefficient')
plt.show()

price_matrix=lnClose.as_matrix()
model = ARIMA(price_matrix, order=(0,1,0))
model_fit = model.fit(disp=0)
from pandas import DataFrame
residuals = DataFrame(model_fit.resid)
residuals.plot()
plt.xlabel('day number starting with 0')
plt.ylabel('residuals')
plt.show()
res = residuals.describe()
print(residuals)

predictions=model_fit.predict(1, 2511, typ='levels')
# print(predictions)
predictionsadjusted=np.exp(predictions)
# print(predictionsadjusted)

tmp = []
residuals = [];

predictions = [np.mean(predictions)] + predictions
        
residual = pd.Series(np.array(predictionsadjusted) - np.array(predictions))
# print("resi")
# print(residual)
tmp.append(np.array(residual))

residuals.append(tmp[0][:2511])
print("Fitted data: ")
print(residuals[0][:20])

output = DataFrame(residuals[0][:2511])
output.plot()
plt.xlabel('day number starting with 0')
plt.ylabel('new closing price')
plt.show()

residuals = pd.DataFrame(residuals)

import csv

np.savetxt("/content/residuals.csv", residuals, delimiter='\n', header="Residuals", comments="")

import sys
import warnings

if not sys.warnoptions:
    warnings.simplefilter('ignore')

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from datetime import datetime
from datetime import timedelta
from tqdm import tqdm
sns.set()
tf.compat.v1.random.set_random_seed(1234)

df = pd.read_csv('/content/residuals.csv')
df.head()

minmax = MinMaxScaler().fit(df.iloc[:, 0:1].astype('float32')) # Residual index
df_log = minmax.transform(df.iloc[:, 0:1].astype('float32')) # Residual index
df_log = pd.DataFrame(df_log)
df_log.head()

output = DataFrame(df_log)
output.plot()
plt.xlabel('day number starting with 0')
plt.ylabel('Scaled down closing price')
plt.show()

test_size = 20
simulation_size = 1

df_train = df_log.iloc[:-test_size]
df_test = df_log.iloc[-test_size:]
df.shape, df_train.shape, df_test.shape

class Model:
    def __init__(
        self,
        learning_rate,
        num_layers,
        size,
        size_layer,
        output_size,
        forget_bias = 0.1,
    ):
        def lstm_cell(size_layer):
            return tf.nn.rnn_cell.GRUCell(size_layer)

        rnn_cells = tf.nn.rnn_cell.MultiRNNCell(
            [lstm_cell(size_layer) for _ in range(num_layers)],
            state_is_tuple = False,
        )
        self.X = tf.placeholder(tf.float32, (None, None, size))
        self.Y = tf.placeholder(tf.float32, (None, output_size))
        drop = tf.contrib.rnn.DropoutWrapper(
            rnn_cells, output_keep_prob = forget_bias
        )
        self.hidden_layer = tf.placeholder(
            tf.float32, (None, num_layers * size_layer)
        )
        self.outputs, self.last_state = tf.nn.dynamic_rnn(
            drop, self.X, initial_state = self.hidden_layer, dtype = tf.float32
        )
        self.logits = tf.layers.dense(self.outputs[-1], output_size)
        self.cost = tf.reduce_mean(tf.square(self.Y - self.logits))
        self.optimizer = tf.train.AdamOptimizer(learning_rate).minimize(
            self.cost
        )

from sklearn.metrics import mean_squared_error

def calculate_accuracy(real, predict):
        real = np.array(real) + 1
        predict = np.array(predict) + 1
        #error = mean_squared_error(real, predict)
        percentage = 1 - np.sqrt(np.mean(np.square((real - predict) / real)))
        return percentage * 100

def anchor(signal, weight):
    buffer = []
    last = signal[0]
    for i in signal:
        smoothed_val = last * weight + (1 - weight) * i
        buffer.append(smoothed_val)
        last = smoothed_val
    return buffer

num_layers = 1
size_layer = 128
timestamp = 5
epoch = 1
dropout_rate = 0.8
future_day = test_size
learning_rate = 0.01

def forecast():
    tf.reset_default_graph()
    modelnn = Model(0.01, num_layers, df_log.shape[1], size_layer, df_log.shape[1], dropout_rate)
    sess = tf.InteractiveSession()
    sess.run(tf.global_variables_initializer())
    date_ori = pd.to_datetime(df.iloc[:, 0]).tolist()

    pbar = tqdm(range(epoch), desc = 'train loop')
    for i in pbar:
        init_value = np.zeros((1, num_layers * size_layer))
        total_loss, total_acc = [], []
        for k in range(0, df_train.shape[0] - 1, timestamp):
            index = min(k + timestamp, df_train.shape[0] - 1)
            batch_x = np.expand_dims(
                df_train.iloc[k : index, :].values, axis = 0
            )
            batch_y = df_train.iloc[k + 1 : index + 1, :].values
            logits, last_state, _, loss = sess.run(
                [modelnn.logits, modelnn.last_state, modelnn.optimizer, modelnn.cost],
                feed_dict = {
                    modelnn.X: batch_x,
                    modelnn.Y: batch_y,
                    modelnn.hidden_layer: init_value,
                },
            )        
            init_value = last_state
            total_loss.append(loss)
            total_acc.append(calculate_accuracy(batch_y[:, 0], logits[:, 0]))
        pbar.set_postfix(cost = np.mean(total_loss), acc = np.mean(total_acc))
    
    future_day = test_size

    output_predict = np.zeros((df_train.shape[0] + future_day, df_train.shape[1]))
    output_predict[0] = df_train.iloc[0]
    upper_b = (df_train.shape[0] // timestamp) * timestamp
    init_value = np.zeros((1, num_layers * size_layer))

    for k in range(0, (df_train.shape[0] // timestamp) * timestamp, timestamp):
        out_logits, last_state = sess.run(
            [modelnn.logits, modelnn.last_state],
            feed_dict = {
                modelnn.X: np.expand_dims(
                    df_train.iloc[k : k + timestamp], axis = 0
                ),
                modelnn.hidden_layer: init_value,
            },
        )
        init_value = last_state
        output_predict[k + 1 : k + timestamp + 1] = out_logits

    if upper_b != df_train.shape[0]:
        out_logits, last_state = sess.run(
            [modelnn.logits, modelnn.last_state],
            feed_dict = {
                modelnn.X: np.expand_dims(df_train.iloc[upper_b:], axis = 0),
                modelnn.hidden_layer: init_value,
            },
        )
        output_predict[upper_b + 1 : df_train.shape[0] + 1] = out_logits
        future_day -= 1
        date_ori.append(date_ori[-1] + timedelta(days = 1))

    init_value = last_state
    
    for i in range(future_day):
        o = output_predict[-future_day - timestamp + i:-future_day + i]
        out_logits, last_state = sess.run(
            [modelnn.logits, modelnn.last_state],
            feed_dict = {
                modelnn.X: np.expand_dims(o, axis = 0),
                modelnn.hidden_layer: init_value,
            },
        )
        init_value = last_state
        output_predict[-future_day + i] = out_logits[-1]
        date_ori.append(date_ori[-1] + timedelta(days = 1))
    
    output_predict = minmax.inverse_transform(output_predict)
    deep_future = anchor(output_predict[:, 0], 0.3)
    
    return deep_future[-test_size:]

results = []
for i in range(simulation_size):
    print('simulation %d'%(i+1))
    results.append(forecast())

#accuracies = [calculate_accuracy(df['Residuals'].iloc[-test_size:].values, r) for r in results]

accuracies = [calculate_accuracy(variables ["Close"].iloc[-test_size:].values, r) for r in results]

#plt.figure(figsize = (7, 5))
#for no, r in enumerate(results):
    #plt.plot(r, label = 'forecast')
    #print(r)
    
#plt.plot(variables ["Close"].iloc[-test_size:].values, label = 'true trend', c = 'black')
#plt.legend()
# plt.xlabel('Last 20 days number')
# plt.ylabel('Closing price')
# plt.plot([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,18, 19, 20], [306.24094273393814, 298.93582946247506, 297.88738043002775, 311.49470617045796, 309.77614429129363, 295.29370523206046, 304.9302550797593, 315.2214213683863, 312.7705063476503, 294.4208456538196, 297.05613524706513, 297.5264615781839, 303.410048230613, 303.6669650722423, 298.85978155635894, 295.1496551384521, 306.3445874531751, 315.62318332657355, 313.756858753071, 315.7873067899246]
# , label='Original closing prices')
# plt.plot([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,18, 19, 20], [318.450012, 298.100006, 306.25,311,321.950012,320.549988,318.5,324.350006,320.200012,327.450012,319.399994,314.200012,317.549988,320.350006,327.649994,322.950012,326.799988, 328.200012, 321.950012,303], label='Predicted closing prices')
# plt.legend()

print("Accuracy : ", np.mean(accuracies), " %")
plt.show()